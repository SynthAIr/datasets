{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assembling the json files into a better format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iterate in the data/ folder with glob\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "import pickle\n",
    "import shutil\n",
    "from tqdm import tqdm\n",
    "\n",
    "def get_longest_paths(root):\n",
    "    paths = []\n",
    "    for root, dirs, files in os.walk(root):\n",
    "        for name in dirs:\n",
    "            paths.append(os.path.join(root, name))\n",
    "\n",
    "    paths = [path for path in paths if 'consolidated' not in path]\n",
    "    max_length = max(len(path) for path in paths)\n",
    "    longest_paths = [path for path in paths if len(path) == max_length]\n",
    "\n",
    "    return longest_paths\n",
    "\n",
    "def get_file_names(path):\n",
    "    return [name for name in os.listdir(path) if os.path.isfile(os.path.join(path, name))]\n",
    "\n",
    "def extract_timestamp(filepath):\n",
    "    # Split the filepath into parts\n",
    "    parts = filepath.split('/')\n",
    "    \n",
    "    # Extract the date and time parts\n",
    "    date_parts = parts[-4:-1]  # ['2020', '03', '25']\n",
    "    time_part = parts[-1].split('.')[0]  # '013200Z'\n",
    "    \n",
    "    # Combine the date and time parts into a timestamp\n",
    "    timestamp_str = '-'.join(date_parts) + ' ' + time_part[:-1]\n",
    "    \n",
    "    # Convert the timestamp string to a pandas Timestamp object\n",
    "    timestamp = pd.to_datetime(timestamp_str, format='%Y-%m-%d %H%M%S')\n",
    "    \n",
    "    return timestamp\n",
    "\n",
    "def save_file(filepath, data:pd.DataFrame) -> None:\n",
    "    with open(filepath, 'wb') as file:\n",
    "        pickle.dump(data, file)\n",
    "\n",
    "def load_file(filepath) -> pd.DataFrame:\n",
    "    with open(filepath, 'rb') as file:\n",
    "        data = pickle.load(file)\n",
    "    return data\n",
    "\n",
    "def clean_root(root_dir):\n",
    "    dirs = os.listdir(root_dir)\n",
    "    for dir in dirs:\n",
    "        if dir != 'consolidated':\n",
    "            shutil.rmtree(os.path.join(root_dir, dir))\n",
    "\n",
    "# Call the function\n",
    "longest_paths = get_longest_paths('data/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_df(data):\n",
    "    df = pd.json_normalize(data['aircraft'])\n",
    "    df['timestamp'] = pd.to_datetime(data['now'], unit='s')\n",
    "    df.set_index('timestamp', inplace=True)\n",
    "    df = df.replace({None: np.nan})\n",
    "    # df = df.astype({\n",
    "    #     'flight': 'object',\n",
    "    #     'type': 'object',\n",
    "    #     'hex': 'object',\n",
    "    #     'r': 'object',\n",
    "    #     't': 'object',\n",
    "    #     'gs': 'float64',\n",
    "    #     'track': 'float64',\n",
    "    #     'baro_rate': 'float64',\n",
    "    #     'alt_geom': 'float64'\n",
    "    # })\n",
    "    def convert_alt_baro(value):\n",
    "        try:\n",
    "            return float(value)\n",
    "        except (TypeError, ValueError):\n",
    "            return float(-1)\n",
    "\n",
    "    # Apply the conversion function\n",
    "    df['alt_baro'] = df['alt_baro'].apply(convert_alt_baro)\n",
    "    df = df.sort_index()\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def logic(root):\n",
    "    longest_paths = get_longest_paths(root)\n",
    "    for path in tqdm(sorted(longest_paths), desc='Processing ...'):\n",
    "        file_names = get_file_names(path)\n",
    "        dfs = []\n",
    "        # for file in sorted(file_names):\n",
    "        for file in sorted(file_names):\n",
    "            with open(os.path.join(path, file), 'r') as f:\n",
    "                data = json.load(f)\n",
    "            dfs.append(create_df(data))\n",
    "        df = pd.concat(dfs)\n",
    "        # Saving df to hdf file\n",
    "        timestamp = df.iloc[0].name\n",
    "        year = str(timestamp.year)\n",
    "        month = str(timestamp.month).zfill(2)\n",
    "        os.makedirs(f\"data/consolidated/{year}\", exist_ok=True)\n",
    "        f_name = f\"data/consolidated/{year}/{month}.pkl\"\n",
    "\n",
    "        with open(f_name, 'wb') as f:\n",
    "            pickle.dump(df, f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing ...:   0%|          | 0/42 [00:03<?, ?it/s]\n"
     ]
    }
   ],
   "source": [
    "logic('data')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alt_baro -> float64\n",
    "squawk -> int\n",
    "lat -> float64\n",
    "lon -> float64\n",
    "seen_pos -> timestamp\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "path = 'data/2021/01/01'\n",
    "for root, dirs, file_names in os.walk(path):\n",
    "    dfs = []\n",
    "    for file in sorted(file_names):\n",
    "        with open(os.path.join(path, file), 'r') as f:\n",
    "            data = json.load(f)\n",
    "        dfs.append(create_df(data))\n",
    "        df = pd.concat(dfs)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
